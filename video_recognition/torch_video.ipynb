{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import av\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from timm.data import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from vidaug import augmentors as va\n",
    "\n",
    "from transforms import RandomCutmix, RandomMixup\n",
    "\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "scaler = GradScaler()\n",
    "tb_writer = SummaryWriter(log_dir=\"runs/exp\")\n",
    "\n",
    "root_dir = '../data/sibur_data/'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "batch_size = 48\n",
    "n_frames = 4\n",
    "N_CLASSES = 4\n",
    "\n",
    "USE_EMA = False\n",
    "world_size = 1\n",
    "model_ema_steps=32\n",
    "model_ema_decay = 0.99998\n",
    "\n",
    "MIXUP_ALPHA = 0.2\n",
    "CUTMIX_ALPHA = 1.0\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path: Path, img_size=None, n_frames=None):\n",
    "    cpr = cv2.VideoCapture(video_path.as_posix())\n",
    "    has_frame = True\n",
    "    frames = []\n",
    "\n",
    "    while has_frame:\n",
    "        has_frame, frame = cpr.read()\n",
    "        if has_frame:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            if img_size:\n",
    "                frame = cv2.resize(frame, img_size)\n",
    "            frames.append(frame)\n",
    "    cpr.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialMovingAverage(torch.optim.swa_utils.AveragedModel):\n",
    "    \"\"\"Maintains moving averages of model parameters using an exponential decay.\n",
    "    ``ema_avg = decay * avg_model_param + (1 - decay) * model_param``\n",
    "    `torch.optim.swa_utils.AveragedModel <https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies>`_\n",
    "    is used to compute the EMA.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, decay, device=\"cpu\"):\n",
    "        def ema_avg(avg_model_param, model_param, num_averaged):\n",
    "            return decay * avg_model_param + (1 - decay) * model_param\n",
    "\n",
    "        super().__init__(model, device, ema_avg, use_buffers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_video_augmentations(video, transform):\n",
    "    targets={'image': video[0]}\n",
    "    for i in range(1, video.shape[0]):\n",
    "        targets[f'image{i}'] = video[i]\n",
    "    transformed = transform(**targets)\n",
    "    transformed = np.concatenate(\n",
    "        [np.expand_dims(transformed['image'], axis=0)] \n",
    "        + [np.expand_dims(transformed[f'image{i}'], axis=0) for i in range(1, video.shape[0])]\n",
    "    )\n",
    "    return transformed\n",
    "\n",
    "def apply_video_augmentations_torch(video, transform):\n",
    "    targets={'image': video[0]}\n",
    "    for i in range(1, video.shape[0]):\n",
    "        targets[f'image{i}'] = video[i]\n",
    "    transformed = transform(**targets)\n",
    "    transformed = torch.cat(\n",
    "        [transformed['image'][None]] \n",
    "        + [transformed[f'image{i}'][None] for i in range(1, video.shape[0])]\n",
    "    )\n",
    "    transformed = transformed.permute(1, 0, 2, 3) # (batch,seq,ch,w,h) -> (batch,ch,seq,w,h)\n",
    "    return transformed\n",
    "\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, seg_len):\n",
    "    start_idx, end_idx = 0, seg_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"bridge_down\", 1: \"bridge_up\", 2: \"no_action\", 3: \"train_in_out\"}\n",
    "label2id = {l:i for i, l in id2label.items()}\n",
    "labels = list(id2label.values())\n",
    "\n",
    "video_paths = list(Path(root_dir).rglob(\"*.mp4\"))\n",
    "targets = [vp.parent.name for vp in video_paths]\n",
    "train = pd.DataFrame({\n",
    "    \"video_path\": [v.as_posix() for v in video_paths],\n",
    "    \"label\": targets,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label_id'] = train.label.map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# X_train, X_val, _, _ = train_test_split(train, train['label'], test_size=0.1, random_state=42)\n",
    "\n",
    "# X_train.to_csv(\"train.csv\", index=False)\n",
    "# X_val.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "# 2\n",
    "# X_train = pd.read_csv(\"train.csv\")\n",
    "# X_val = pd.read_csv(\"test.csv\")\n",
    "\n",
    "###\n",
    "X_train = X_val = train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.video.mvit_v2_s(\"DEFAULT\")\n",
    "# model.head[1] = torch.nn.Linear(model.head[1].in_features, 4)\n",
    "# model.to(device)\n",
    "\n",
    "model = models.video.swin3d_t(\"DEFAULT\")\n",
    "model.head = torch.nn.Linear(model.head.in_features, N_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "# model = models.video.s3d(\"DEFAULT\")\n",
    "# model.classifier[1] = torch.nn.Conv3d(model.classifier[1].in_channels, N_CLASSES, kernel_size=1, stride=1)\n",
    "# model.to(device)\n",
    "\n",
    "# model = models.video.r2plus1d_18(\"DEFAULT\")\n",
    "# model.fc = torch.nn.Linear(model.fc.in_features, N_CLASSES)\n",
    "# model.to(device)\n",
    "\n",
    "# model = models.video.swin3d_s(\"DEFAULT\")\n",
    "# model.head = torch.nn.Linear(model.head.in_features, N_CLASSES)\n",
    "# model.to(device)\n",
    "\n",
    "# model = models.video.mc3_18(\"DEFAULT\")\n",
    "# model.fc = torch.nn.Linear(model.fc.in_features, N_CLASSES)\n",
    "# model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(224, 224, scale=(0.4, 1.0)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5\n",
    "    ),\n",
    "    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.OneOf([  # One of blur or adding gauss noise\n",
    "        A.Blur(blur_limit=4, p=0.5),  # Blurs the image\n",
    "        A.GaussNoise(var_limit=5.0 / 255.0, p=0.5)  # Adds Gauss noise to image\n",
    "    ], p=0.5),\n",
    "    A.HueSaturationValue(p=0.5),\n",
    "    A.Cutout(num_holes=8, p=0.15),\n",
    "    A.Normalize(OPENAI_CLIP_MEAN, OPENAI_CLIP_STD),\n",
    "    ToTensorV2(),\n",
    "], additional_targets={\n",
    "    f'image{i}': 'image'\n",
    "    for i in range(1, n_frames)\n",
    "})\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.CenterCrop(224, 224),\n",
    "    A.Normalize(OPENAI_CLIP_MEAN, OPENAI_CLIP_STD),\n",
    "    ToTensorV2(),\n",
    "], additional_targets={\n",
    "    f'image{i}': 'image'\n",
    "    for i in range(1, n_frames)\n",
    "})\n",
    "\n",
    "# VIDEO AUGS\n",
    "sometimes = lambda aug: va.Sometimes(0.5, aug)\n",
    "video_aug = va.Sequential([\n",
    "    va.OneOf([\n",
    "        va.Upsample(1.3),\n",
    "        va.Downsample(0.7)\n",
    "    ]),\n",
    "    sometimes(va.OneOf([\n",
    "        va.Pepper(),\n",
    "        va.Salt(),\n",
    "    ])),\n",
    "    sometimes(va.RandomShear(0.1, 0.1))\n",
    "])\n",
    "\n",
    "\n",
    "# MIXUP CUTMIX\n",
    "mix_transforms = []\n",
    "if MIXUP_ALPHA:\n",
    "    mix_transforms.append(RandomMixup(N_CLASSES, p=1.0, alpha=MIXUP_ALPHA))\n",
    "if CUTMIX_ALPHA:\n",
    "    mix_transforms.append(RandomCutmix(N_CLASSES, p=1.0, alpha=CUTMIX_ALPHA))\n",
    "\n",
    "mixup_cutmix = torchvision.transforms.RandomChoice(mix_transforms)\n",
    "def collate_fn(batch):\n",
    "    return mixup_cutmix(*default_collate(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, meta, stage, transform=None, n_frames=16):\n",
    "        self.meta = meta\n",
    "        self.transform = transform\n",
    "        self.n_frames = n_frames\n",
    "        self.stage = stage\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample_frame_indices_with_frame_rate(clip_len, frame_sample_rate, seg_len):\n",
    "        converted_len = int(clip_len * frame_sample_rate)\n",
    "        try:\n",
    "            end_idx = np.random.randint(converted_len, seg_len)\n",
    "        except:\n",
    "            end_idx = seg_len - 1\n",
    "        start_idx = end_idx - converted_len\n",
    "        indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "        return indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        file_path = self.meta['video_path'].iloc[idx]\n",
    "        container = av.open(file_path)\n",
    "        frame_sample_rate = np.random.choice([5, 10, 15])\n",
    "        indices = self._sample_frame_indices_with_frame_rate(\n",
    "            clip_len=int(container.streams.video[0].frames / frame_sample_rate), \n",
    "            frame_sample_rate=frame_sample_rate, \n",
    "            seg_len=container.streams.video[0].frames\n",
    "        )\n",
    "        video = read_video_pyav(container, indices)\n",
    "        \n",
    "        if self.stage == \"train\" and len(video) > 8:\n",
    "            video = np.array(video_aug(video))\n",
    "\n",
    "        if self.stage == \"train\":\n",
    "            video = np.array(video_aug(video))\n",
    "            seg_len = len(video)\n",
    "            # mask indices\n",
    "            # в тесте 5/6 всех видео замаскированы\n",
    "            if np.random.random() < 0.8 and seg_len > 8: \n",
    "                first_idxs = np.random.choice(\n",
    "                    range(0, seg_len), \n",
    "                    int(seg_len*np.random.uniform(0.5, 0.75)), \n",
    "                    replace=False\n",
    "                ).astype(int)\n",
    "                first_idxs.sort()\n",
    "            else:\n",
    "                first_idxs = np.arange(seg_len)\n",
    "            # first_idxs = np.arange(seg_len)\n",
    "            \n",
    "            # get frames\n",
    "            start_idx = np.random.randint(0, len(first_idxs) // 2)\n",
    "            end_idx = min(np.random.randint(len(first_idxs) // 2, len(first_idxs)) + self.n_frames, len(first_idxs))\n",
    "            indices = np.linspace(start_idx, end_idx, num=self.n_frames)\n",
    "            indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "\n",
    "            indices = first_idxs[indices]\n",
    "            video = video[indices]\n",
    "        else:\n",
    "            indices = sample_frame_indices(clip_len=self.n_frames, seg_len=container.streams.video[0].frames)\n",
    "            video = read_video_pyav(container, indices)\n",
    "                    \n",
    "        while video.shape[0] < self.n_frames:\n",
    "            video = np.vstack([video, video[-1:]])\n",
    "\n",
    "        if self.transform:\n",
    "            video = apply_video_augmentations_torch(video, self.transform)\n",
    "\n",
    "        target = np.zeros(N_CLASSES)\n",
    "        target[self.meta.iloc[idx].label_id] = 1\n",
    "            \n",
    "        return video, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ActionDataset(meta=X_train, stage=\"train\", transform=transform, n_frames=n_frames)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "test_dataset = ActionDataset(meta=X_val, stage=\"test\", transform=transform, n_frames=n_frames)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "warm_epochs = 0\n",
    "ema_warm_epochs = 4\n",
    "lr = 1e-4\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.08)\n",
    "optimizer = optim.AdamW(model.parameters(), lr, weight_decay=2e-05)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ema = None\n",
    "if USE_EMA:\n",
    "    adjust = world_size * batch_size * model_ema_steps / epochs\n",
    "    alpha = 1.0 - model_ema_decay\n",
    "    alpha = min(1.0, alpha * adjust)\n",
    "    model_ema = ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # TRAIN MODEL\n",
    "    model.train()    \n",
    "\n",
    "    if epoch <= warm_epochs:\n",
    "        if epoch < warm_epochs:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in model.head.parameters():\n",
    "                param.requires_grad = True\n",
    "        if epoch == warm_epochs:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    train_loss = []\n",
    "    for i, (batch, target) in enumerate(tqdm(train_dataloader, desc=f\"Epoch: {epoch} (train)\")):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(\"cuda\"):\n",
    "            batch = batch.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            logits = model(batch)\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        # update ema model\n",
    "        if model_ema and i % model_ema_steps == 0:\n",
    "            model_ema.update_parameters(model)\n",
    "            if epoch < ema_warm_epochs:\n",
    "                model_ema.n_averaged.fill_(0)\n",
    "    \n",
    "    # EVAL MODEL \n",
    "    model.eval()  \n",
    "\n",
    "    val_targets = []\n",
    "    val_preds = []\n",
    "    val_loss = 0\n",
    "    for i, (batch, target) in enumerate(tqdm(test_dataloader, desc=f\"Epoch: {epoch} (eval)\")):\n",
    "\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            batch = batch.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(batch)\n",
    "                loss = criterion(logits, target)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_targets.extend(target.argmax(1).cpu().tolist())\n",
    "        val_preds.extend(logits.argmax(1).cpu().tolist())\n",
    "\n",
    "    val_loss /= len(test_dataloader)\n",
    "    val_score = round(f1_score(val_targets, val_preds, average='macro'), 4)\n",
    "\n",
    "    # EVAL EMA MODEL\n",
    "    if epoch >= ema_warm_epochs and model_ema is not None:\n",
    "        model_ema.eval()  \n",
    "\n",
    "        val_targets = []\n",
    "        val_preds = []\n",
    "        ema_val_loss = 0\n",
    "        for i, (batch, target) in enumerate(tqdm(test_dataloader, desc=f\"Epoch: {epoch} (ema)\")):\n",
    "\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                batch = batch.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = model_ema(batch)\n",
    "                    loss = criterion(logits, target)\n",
    "\n",
    "            ema_val_loss += loss.item()\n",
    "            val_targets.extend(target.argmax(1).cpu().tolist())\n",
    "            val_preds.extend(logits.argmax(1).cpu().tolist())\n",
    "\n",
    "        ema_val_loss /= len(test_dataloader)\n",
    "        ema_val_score = round(f1_score(val_targets, val_preds, average='macro'), 4)\n",
    "\n",
    "    # LOG RESULTS\n",
    "    print(f'Training loss: {np.mean(train_loss):.4f}')\n",
    "    print(f'Valindation loss: {val_loss:.4f}')\n",
    "    tb_writer.add_scalar(\"loss/train\", np.mean(train_loss), epoch)\n",
    "    tb_writer.add_scalar(\"loss/val\", val_loss, epoch)\n",
    "    if model_ema is not None and epoch >= ema_warm_epochs:\n",
    "            print(f'Valindation loss (EMA): {ema_val_loss:.4f}')\n",
    "            tb_writer.add_scalar(\"loss/ema\", ema_val_loss, epoch)\n",
    "    print('F1:', val_score)\n",
    "    \n",
    "    tb_writer.add_scalar(\"f1/val\", val_score, epoch)\n",
    "    if epoch >= ema_warm_epochs and model_ema is not None:\n",
    "        print('F1 (EMA):', ema_val_score)\n",
    "        tb_writer.add_scalar(\"f1/ema\", val_score, epoch)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model, \"best.pt\")\n",
    "        if epoch >= ema_warm_epochs and model_ema is not None:\n",
    "            torch.save(model_ema.state_dict(), \"best_ema.pt\")\n",
    "\n",
    "    torch.save(model, \"last.pt\")\n",
    "    if epoch > ema_warm_epochs and model_ema is not None:\n",
    "        torch.save(model_ema.state_dict(), \"last_ema.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to onnx & openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss: 0.3523\n",
    "# Valindation loss: 0.3076\n",
    "# Valindation loss (EMA): 0.3470\n",
    "# F1: 1.0\n",
    "# F1 (EMA): 0.9871\n",
    "\n",
    "X_val = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = \"last\"\n",
    "model = torch.load(f\"{mt}.pt\")\n",
    "\n",
    "if USE_EMA:\n",
    "    adjust = world_size * batch_size * model_ema_steps / epochs\n",
    "    alpha = 1.0 - model_ema_decay\n",
    "    alpha = min(1.0, alpha * adjust)\n",
    "    model_ema = ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "    ema_weights = torch.load(f\"{mt}_ema.pt\", map_location=\"cpu\")\n",
    "    model_ema.load_state_dict(ema_weights)\n",
    "    model = model_ema\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "file_path = X_val.iloc[0].video_path\n",
    "container = av.open(file_path)\n",
    "indices = sample_frame_indices(clip_len=n_frames, seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container, indices)\n",
    "inputs = apply_video_augmentations_torch(video, transform).unsqueeze(0)\n",
    "\n",
    "outputs = model(inputs.to(device)).cpu()\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "onnx_path = \"../submit_videorec/model/model.onnx\"\n",
    "openvino_path = \"../submit_videorec/model/model.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float().cpu()\n",
    "model.eval()\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    inputs,\n",
    "    onnx_path,  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"input\"],  # the model's input names\n",
    "    output_names=['output'],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"input\": {0: \"batch\", 1: \"channels\", 2: \"sequence\"},\n",
    "        \"output\": {0: \"batch\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import serialize\n",
    "from openvino.tools import mo\n",
    "\n",
    "ov_model = mo.convert_model(onnx_path, compress_to_fp16=True)\n",
    "serialize(ov_model, openvino_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "test_videos_paths = [Path(p) for p in X_val.video_path.values][:]\n",
    "test_targets = X_val.label_id[:]\n",
    "test_targets_masked = [t for t in test_targets for _ in range(6)]\n",
    "\n",
    "clips = [\n",
    "    get_frames(vp)\n",
    "    for vp in tqdm(test_videos_paths[:])\n",
    "]\n",
    "\n",
    "masks = [\n",
    "    np.ones(len(c), dtype=bool)\n",
    "    for c in clips\n",
    "]\n",
    "\n",
    "new_masks = []\n",
    "for m in masks:\n",
    "    new_masks.append(m)\n",
    "    for _ in range(5):\n",
    "        new_m = m.copy()\n",
    "        new_m[np.random.choice(range(0, len(m)), int(len(m)*0.4), replace=False).astype(int)] = False\n",
    "        new_masks.append(new_m)\n",
    "\n",
    "masked_clips = []\n",
    "for i, c in enumerate(clips):\n",
    "    for m in new_masks[i*6:(i+1)*6]:\n",
    "        masked_clips.append(c[m])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onnx/openvino model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from submit_videorec.predict_openvino import predict as predict_openvino\n",
    "from submit_videorec.predict_onnx import predict as predict_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [\n",
    "    predict_onnx(clip)\n",
    "    for clip in tqdm(masked_clips)\n",
    "]\n",
    "preds_ids = [label2id[i] for i in preds]\n",
    "round(f1_score(test_targets_masked, preds_ids, average='macro'), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [\n",
    "    predict_openvino(clip)\n",
    "    for clip in tqdm(masked_clips)\n",
    "]\n",
    "preds_ids = [label2id[i] for i in preds]\n",
    "round(f1_score(test_targets_masked, preds_ids, average='macro'), 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sibur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
